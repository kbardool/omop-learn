{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "import psutil\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from ipywidgets import IntProgress, FloatText\n",
    "from IPython.display import display\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from Utils.ORM.postgres_db import PostgresDatabase\n",
    "import Utils.ORM.data_utils as data_utils\n",
    "from Generators.ORM.EolCohortGenerator import EolCohortTable\n",
    "from Generators.ORM.FeatureGenerator import FeatureSet, postprocess_feature_matrix\n",
    "import Models.LogisticRegression.RegressionGen as lr_models\n",
    "import Models.Transformer.visit_transformer as visit_transformer\n",
    "from getpass import getpass\n",
    "from config import user_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "visit_transformer = importlib.reload(visit_transformer)\n",
    "data_utils = importlib.reload(data_utils)\n",
    "assert(torch.cuda.is_available())\n",
    "torch.cuda.set_device(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = 'orm_eol'\n",
    "mn_prefix = datetime.now().strftime('%Y%m%dT%H%M%S')\n",
    "print('This run:', mn_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_name = 'pgsql01/ds_omop_cdm'\n",
    "schema_name = user_schema\n",
    "username = 'ibx8568'\n",
    "password = getpass()\n",
    "db = PostgresDatabase(username, password, database_name, schema_name)\n",
    "remake_cohort = False\n",
    "use_cached_features = False\n",
    "del password"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cohort definition parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort_args = {\n",
    "    'training_start_date':datetime.strptime('2016-01-01', '%Y-%m-%d'),\n",
    "    'training_end_date':datetime.strptime('2017-01-01', '%Y-%m-%d'),\n",
    "    'gap_months':3,\n",
    "    'outcome_months':9,\n",
    "    'min_enroll_proportion':0.95\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define our cohort table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tic = time.perf_counter()\n",
    "cohort = EolCohortTable(**cohort_args)\n",
    "cohort.build(db, remake_cohort)\n",
    "toc = time.perf_counter()\n",
    "print('Cohort build took {:,.2f} seconds'.format(toc - tic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureSet = FeatureSet(db, EolCohortTable)\n",
    "featureSet.add_default_features(['Drugs','Conditions','Procedures','Specialty'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "featureSet.build(cohort, from_cached=use_cached_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.session.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcomes_filt, feature_matrix_3d_transpose, remap, good_feature_names = postprocess_feature_matrix(cohort, featureSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process data for deep model\n",
    "person_ixs, time_ixs, code_ixs = feature_matrix_3d_transpose.coords\n",
    "all_codes_tensor = code_ixs\n",
    "people = sorted(np.unique(person_ixs))\n",
    "person_indices = np.searchsorted(person_ixs, people)\n",
    "person_indices = np.append(person_indices, len(person_ixs))\n",
    "person_chunks = [\n",
    "    time_ixs[person_indices[i]: person_indices[i + 1]]\n",
    "    for i in range(len(person_indices) - 1)\n",
    "]\n",
    "\n",
    "visit_chunks = []\n",
    "visit_times_raw = []\n",
    "\n",
    "for i, chunk in enumerate(person_chunks):\n",
    "    visits = sorted(np.unique(chunk))\n",
    "    visit_indices_local = np.searchsorted(chunk, visits)\n",
    "    visit_indices_local = np.append(\n",
    "        visit_indices_local,\n",
    "        len(chunk)\n",
    "    )\n",
    "    visit_chunks.append(visit_indices_local)\n",
    "    visit_times_raw.append(visits)\n",
    "\n",
    "n_visits = {i:len(j) for i,j in enumerate(visit_times_raw)}\n",
    "\n",
    "visit_days_rel = {\n",
    "    i: (\n",
    "        cohort_args['training_end_date'] \\\n",
    "        - pd.to_datetime(featureSet.time_map[i])\n",
    "    ).days for i in featureSet.time_map\n",
    "}\n",
    "vdrel_func = np.vectorize(visit_days_rel.get)\n",
    "visit_time_rel = [\n",
    "    vdrel_func(v) for v in visit_times_raw\n",
    "]\n",
    "\n",
    "maps = {\n",
    "    'concept': featureSet.concept_map,\n",
    "    'id': featureSet.id_map,\n",
    "    'time': featureSet.time_map\n",
    "}\n",
    "\n",
    "dataset_dict = {\n",
    "    'all_codes_tensor': all_codes_tensor, # A tensor of all codes occurring in the dataset\n",
    "    'person_indices': person_indices, # A list of indices such that all_codes_tensor[person_indices[i]: person_indices[i+1]] are the codes assigned to the ith patient\n",
    "    'visit_chunks': visit_chunks, # A list of indices such that all_codes_tensor[person_indices[i]+visit_chunks[j]:person_indices[i]+visit_chunks[j+1]] are the codes assigned to the ith patient during their jth visit\n",
    "    'visit_time_rel': visit_time_rel, # A list of times (as measured in days to the prediction date) for each visit\n",
    "    'n_visits': n_visits, # A dict defined such that n_visits[i] is the number of visits made by the ith patient\n",
    "    'outcomes_filt': outcomes_filt, # A pandas Series defined such that outcomes_filt.iloc[i] is the outcome of the ith patient\n",
    "    'remap': remap,\n",
    "    'maps': maps\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the windowed regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect featre names\n",
    "good_feature_names = np.vectorize(dataset_dict['maps']['concept'].get)(\n",
    "    dataset_dict['remap']['concept']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# window the data using the window lengths specified below\n",
    "feature_matrix_counts, feature_names = data_utils.window_data_sorted(\n",
    "    window_lengths = [30, 180, 365, 730, 10000],\n",
    "    feature_matrix = feature_matrix_3d_transpose,\n",
    "    all_feature_names = good_feature_names,\n",
    "    cohort = cohort, featureSet = featureSet\n",
    ")\n",
    "feature_matrix_counts = feature_matrix_counts.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into train, validate and test sets\n",
    "indices_all = range(len(dataset_dict['outcomes_filt']))\n",
    "X_train, X_test, y_train, y_test, indices_train, indices_test = train_test_split(\n",
    "    feature_matrix_counts, dataset_dict['outcomes_filt'], indices_all,\n",
    "    test_size=0.2, random_state=1\n",
    ")\n",
    "val_size = int(X_test.shape[0] * 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the regression model over several choices of regularization parameter\n",
    "reg_lambdas = [2, 0.2, 0.02, 0.002, 0.0002]\n",
    "lr_val_aucs = []\n",
    "for reg_lambda in reg_lambdas:\n",
    "    clf_lr = lr_models.gen_lr_pipeline(reg_lambda)\n",
    "    clf_lr.fit(X_train, y_train)\n",
    "    pred_lr = clf_lr.predict_proba(X_test[:val_size])[:, 1]\n",
    "    lr_val_aucs.append(roc_auc_score(y_test[:val_size], pred_lr))\n",
    "    print('Validation AUC: {0:.3f}'.format(roc_auc_score(y_test[:val_size], pred_lr)))\n",
    "                       \n",
    "clf_lr = lr_models.gen_lr_pipeline(reg_lambdas[np.argmax(lr_val_aucs)])\n",
    "clf_lr.fit(X_train, y_train)\n",
    "pred_lr_all = clf_lr.predict_proba(feature_matrix_counts)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick the model with the best regularization, as measured by validation performance\n",
    "pred_lr = clf_lr.predict_proba(X_test[val_size:])[:, 1]\n",
    "print('Linear Model Test AUC: {0:.3f}'.format(roc_auc_score(y_test[val_size:], pred_lr)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the SARD deep model on the predictive task\n",
    "### 1. Set Model Parameters and Construct the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the same split as before, create train/validate/test batches for the deep model\n",
    "# `mbsz` might need to be decreased based on the GPU's memory and the number of features being used\n",
    "mbsz = 50\n",
    "def get_batches(arr, mbsz=mbsz):\n",
    "    curr, ret = 0, []\n",
    "    while curr < len(arr) - 1:\n",
    "        ret.append(arr[curr : curr + mbsz])\n",
    "        curr += mbsz\n",
    "    return ret\n",
    "\n",
    "p_ranges_train, p_ranges_test = [\n",
    "    get_batches(arr) for arr in (\n",
    "        indices_train, indices_test\n",
    "    )\n",
    "]\n",
    "p_ranges_val = p_ranges_test[:val_size // mbsz]\n",
    "p_ranges_test = p_ranges_test[val_size // mbsz:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick a name for the model (mn_prefix) that will be used when saving checkpoints\n",
    "# Then, set some parameters for SARD. The values below reflect a good starting point that performed well on several tasks\n",
    "model_params = {\n",
    "    'embedding_dim': 300, # Dimension per head of visit embeddings\n",
    "    'n_heads': 2, # Number of self-attention heads\n",
    "    'attn_depth': 2, # Number of stacked self-attention layers\n",
    "    'dropout': 0.05, # Dropout rate for both self-attention and the final prediction layer\n",
    "    'use_mask': True # Only allow visits to attend to other actual visits, not to padding visits\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up fixed model parameters, loss functions, and build the model on the GPU\n",
    "lr = 2e-4\n",
    "n_epochs_pretrain = 20\n",
    "ft_epochs = 5\n",
    "\n",
    "update_every = 500\n",
    "update_mod = update_every // mbsz\n",
    "\n",
    "base_model = visit_transformer.VisitTransformer(\n",
    "    featureSet, **model_params\n",
    ")\n",
    "\n",
    "clf = visit_transformer.VTClassifer(base_model).cuda()\n",
    "\n",
    "clf.bert.set_data(\n",
    "    torch.LongTensor(dataset_dict['all_codes_tensor']).cuda(),\n",
    "    dataset_dict['person_indices'], dataset_dict['visit_chunks'],\n",
    "    dataset_dict['visit_time_rel'], dataset_dict['n_visits']\n",
    ")\n",
    "\n",
    "loss_function_distill = torch.nn.BCEWithLogitsLoss(\n",
    "    pos_weight=torch.FloatTensor([\n",
    "        len(dataset_dict['outcomes_filt']) / dataset_dict['outcomes_filt'].sum() - 1\n",
    "    ]), reduction='sum'\n",
    ").cuda()\n",
    "\n",
    "optimizer_clf = torch.optim.Adam(params=clf.parameters(), lr=lr)\n",
    "\n",
    "def eval_curr_model_on(a):\n",
    "    with torch.no_grad():\n",
    "        preds_test, true_test = [], []\n",
    "        for batch_num, p_range in enumerate(a):\n",
    "            y_pred = clf(p_range) \n",
    "            preds_test += y_pred.tolist()\n",
    "            true_test += list(dataset_dict['outcomes_filt'].iloc[list(p_range)].values)\n",
    "        return roc_auc_score(true_test, preds_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Fit the SARD model to the best windowed linear model (Reverse Distillation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run `n_epochs_pretrain` of Reverse Distillation pretraining\n",
    "val_losses = []\n",
    "progress_bar = IntProgress(min=0, max=int(n_epochs_pretrain * len(p_ranges_train)))\n",
    "batch_loss_disp = FloatText(value=0.0, description='Avg. Batch Loss for Last 50 Batches', disabled=True)\n",
    "time_disp = FloatText(value=0.0, description='Time for Last 50 Batches', disabled=True)\n",
    "\n",
    "display(progress_bar)\n",
    "display(batch_loss_disp)\n",
    "display(time_disp)\n",
    "\n",
    "for epoch in range(n_epochs_pretrain):\n",
    "    t, batch_loss = time.time(), 0\n",
    "    \n",
    "    for batch_num, p_range in enumerate(p_ranges_train):\n",
    "        \n",
    "        if batch_num % 50 == 0:\n",
    "            batch_loss_disp.value = round(batch_loss / 50, 2)\n",
    "            time_disp.value = round(time.time() - t, 2)\n",
    "            t, batch_loss = time.time(), 0\n",
    "            \n",
    "        y_pred = clf(p_range)\n",
    "        loss_distill = loss_function_distill(\n",
    "            y_pred, torch.FloatTensor(pred_lr_all[p_range]).cuda()\n",
    "        )\n",
    "        \n",
    "        batch_loss += loss_distill.item()\n",
    "        loss_distill.backward()\n",
    "        \n",
    "        if batch_num % update_mod == 0:\n",
    "            optimizer_clf.step()\n",
    "            optimizer_clf.zero_grad()\n",
    "        \n",
    "        progress_bar.value = batch_num + epoch * len(p_ranges_train)\n",
    "        \n",
    "    torch.save(\n",
    "        clf.state_dict(),\n",
    "        \"SavedModels/{task}/{mn_prefix}_pretrain_epochs_{epochs}\".format(\n",
    "                task=task, mn_prefix = mn_prefix, epochs = epoch + 1\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    clf.eval()\n",
    "    ckpt_auc = eval_curr_model_on(p_ranges_val)\n",
    "    print('Epochs: {} | Val AUC: {:.6f}'.format(epoch + 1, ckpt_auc))\n",
    "    val_losses.append(ckpt_auc)\n",
    "    clf.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the pretrained model with best validation-set performance\n",
    "best_pretrain_model = \"SavedModels/{task}/{mn_prefix}_pretrain_epochs_{epochs}\".format(\n",
    "        task=task, mn_prefix=mn_prefix, epochs=np.argmax(val_losses) + 1\n",
    "    )\n",
    "print('best pretrained model:', best_pretrain_model)\n",
    "clf.load_state_dict(\n",
    "    torch.load(best_pretrain_model)\n",
    ")\n",
    "torch.save(\n",
    "        clf.state_dict(),\n",
    "        \"SavedModels/{task}/{mn_prefix}_pretrain_epochs_{epochs}\".format(\n",
    "                task=task, mn_prefix = mn_prefix, epochs = 'BEST'\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Fine-tune the SARD model by training to match the actual outcomes on the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up loss functions for fine-tuning. There are two terms:\n",
    "#    - `loss_function_distill`, which penalizes differences between the linear model prediction and SARD's prediction\n",
    "#    - `loss_function_clf`, which penalizes differences between the true outcome and SARD's prediction\n",
    "loss_function_distill = torch.nn.BCEWithLogitsLoss(\n",
    "    pos_weight=torch.FloatTensor([\n",
    "        len(dataset_dict['outcomes_filt']) / dataset_dict['outcomes_filt'].sum() - 1\n",
    "    ]), reduction='sum'\n",
    ").cuda()\n",
    "\n",
    "loss_function_clf = torch.nn.BCEWithLogitsLoss(\n",
    "    pos_weight=torch.FloatTensor([\n",
    "        len(dataset_dict['outcomes_filt']) / dataset_dict['outcomes_filt'].sum() - 1\n",
    "    ]), reduction='sum'\n",
    ").cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# run `ft_epochs` of fine-tuning training, for each of the values of `alpha` below.\n",
    "# Note that `alpha` is the relative weight of `loss_function_distill` as compared to `loss_function_clf`\n",
    "\n",
    "all_pred_models = {}\n",
    "\n",
    "progress_bar = IntProgress(min=0, max=int(ft_epochs * len(p_ranges_train)))\n",
    "batch_loss_disp = FloatText(value=0.0, description='Avg. Batch Loss for Last 50 Batches', disabled=True)\n",
    "time_disp = FloatText(value=0.0, description='Time for Last 50 Batches', disabled=True)\n",
    "\n",
    "display(progress_bar)\n",
    "display(batch_loss_disp)\n",
    "display(time_disp)\n",
    "\n",
    "\n",
    "no_rd = False\n",
    "for alpha in [0,0.05,0.1,0.15,0.2]:\n",
    "\n",
    "    progress_bar.value = 0\n",
    "    \n",
    "    \n",
    "    if no_rd:\n",
    "        pretrained_model_fn = mn_prefix + '_None'\n",
    "        start_model = None\n",
    "        if start_model is None:\n",
    "            base_model = visit_transformer.VisitTransformer(\n",
    "                featureSet, **model_params\n",
    "            )\n",
    "\n",
    "            clf = visit_transformer.VTClassifer(base_model).cuda()\n",
    "\n",
    "            clf.bert.set_data(\n",
    "                torch.LongTensor(dataset_dict['all_codes_tensor']).cuda(),\n",
    "                dataset_dict['person_indices'], dataset_dict['visit_chunks'],\n",
    "                dataset_dict['visit_time_rel'], dataset_dict['n_visits']\n",
    "            )\n",
    "        else:\n",
    "            pretrained_model_path = \"SavedModels/{task}/{start_model}\".format(\n",
    "                task=task, start_model=start_model\n",
    "            )\n",
    "            clf.load_state_dict(torch.load(pretrained_model_path))\n",
    "            \n",
    "    else: \n",
    "        pretrained_model_fn = \"{mn_prefix}_pretrain_epochs_{epochs}\".format(\n",
    "            mn_prefix=mn_prefix, epochs='BEST'\n",
    "        )\n",
    "        pretrained_model_path = \"SavedModels/{task}/{mn_prefix}_pretrain_epochs_{epochs}\".format(\n",
    "            task=task, mn_prefix=mn_prefix, epochs='BEST'\n",
    "        )\n",
    "        clf.load_state_dict(torch.load(pretrained_model_path))\n",
    "    \n",
    "    clf.train()\n",
    "    \n",
    "    optimizer_clf = torch.optim.Adam(params=clf.parameters(), lr=2e-4)\n",
    "\n",
    "    for epoch in range(ft_epochs):\n",
    "\n",
    "        t, batch_loss = time.time(), 0\n",
    "\n",
    "        for batch_num, p_range in enumerate(p_ranges_train):\n",
    "\n",
    "            if batch_num % 50 == 0:\n",
    "                batch_loss_disp.value = round(batch_loss / 50, 2)\n",
    "                time_disp.value = round(time.time() - t, 2)\n",
    "                t, batch_loss = time.time(), 0\n",
    "\n",
    "            y_pred = clf(p_range)\n",
    "            \n",
    "            loss = loss_function_clf(\n",
    "                y_pred,\n",
    "                torch.FloatTensor(dataset_dict['outcomes_filt'].values[p_range]).cuda()\n",
    "            )\n",
    "\n",
    "            loss_distill = loss_distill = loss_function_distill(\n",
    "                y_pred,\n",
    "                torch.FloatTensor(pred_lr_all[p_range]).cuda()\n",
    "            )\n",
    "\n",
    "            batch_loss += loss.item() + alpha * loss_distill.item()\n",
    "            loss_total = loss + alpha * loss_distill\n",
    "            loss_total.backward()\n",
    "            \n",
    "            if batch_num % update_mod == 0:\n",
    "                optimizer_clf.step()\n",
    "                optimizer_clf.zero_grad()\n",
    "\n",
    "            progress_bar.value = batch_num + epoch * len(p_ranges_train)\n",
    "        \n",
    "        saving_fn = \"{pretrain}_alpha_{alpha}_epochs_{epochs}\".format(\n",
    "            task=task, pretrain = pretrained_model_fn, alpha=alpha, epochs = epoch + 1\n",
    "        )\n",
    "        torch.save(\n",
    "            clf.state_dict(),\n",
    "            \"SavedModels/{task}/ablation/{saving_fn}\".format(\n",
    "                    task=task, saving_fn=saving_fn\n",
    "                )\n",
    "            )\n",
    "        \n",
    "        clf.eval()\n",
    "        val_auc = eval_curr_model_on(p_ranges_val)\n",
    "        print('Alpha: {} | Epochs: {} | Val AUC: {:.6f}'.format(alpha, epoch + 1, val_auc))\n",
    "        all_pred_models[saving_fn] = val_auc\n",
    "        clf.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Evaluate the best SARD model, as determined by validation performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = max(all_pred_models, key=all_pred_models.get)\n",
    "print('best fine tuned model:', best_model)\n",
    "clf.load_state_dict(\n",
    "    torch.load(\"SavedModels/{task}/ablation/{model}\".format(\n",
    "        task=task, model=best_model\n",
    "    ))\n",
    ")\n",
    "clf.eval();\n",
    "with torch.no_grad():\n",
    "    preds_test, true_test = [], []\n",
    "    for batch_num, p_range in enumerate(p_ranges_test):\n",
    "        y_pred = clf(p_range) \n",
    "        preds_test += y_pred.tolist()\n",
    "        true_test += list(dataset_dict['outcomes_filt'].iloc[list(p_range)].values)\n",
    "    print('SARD final Test AUC: {:.6f}'.format(roc_auc_score(true_test, preds_test)))\n",
    "clf.train();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
